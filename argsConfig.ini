[Model]
dropout_input = 0.1
# Small == 256, Large == 1024
feature_num = 256
visualize_model_graph = False

[Data]
# only for server
;dataset = /home/space/datasets/ag_news
#/home/space/datasets/yelp/

dataset = datasets/yelp
#yelp
chunk_size = 10000
encoding = utf-8
max_csv_rows = 6000
csv_sep = ,
balance_classes = True
ratio = 1
preprocess_data = True
steps = None
#steps = remove_hashtags,remove_urls,remove_user_mentions,lower,double_new_line,wordnet_augment_text
usecols = 0,1
#0,1,2 for ag news
#0,1 for yelp
# kFoldCrossValidation is applied if k_folds > 1
k_folds = 5

[DataSet]
alphabet = abcdefghijklmnopqrstuvwxyz0123456789
    -,;.!?:'"/\|_@#$%%^&*~`+-=<>()[]{}
char_num = 70
l0 = 1014

[Train]
batch_size = 128
train_size = 0.90
dev_size = 0.10
use_sampler = True
workers = 6
# celoss, nllloss
criterion = nllloss
binary_cross_entropy_type = anormal
#Adam, SGD, ASGD
optimizer = SGD
# step, clr
scheduler = step
clr_step_size = 2
clr_min_lr = 0.0017
clr_max_lr = 0.01
epochs = 25
max_norm = 400
lr = 0.01
continue_from_checkpoint = False
dynamic_lr = True
milestones = 10,15,20
decay_factor = 0.5
early_stopping = False
patience =
# print train status every (epoch % print_out_every == 0)
print_out_every = 30

[Test]
model_log_dir = None
model_to_test = 
loss_data_path = loss_results

[Log]
# You need to be careful we using the following options
# some options activated together will give you headache
flush_history = False
# for server only
;log_path = /home/pml_28/MS1/logs/
;output = /h/home/pml_28/MS1/logs/

log_path = logs/
output = models/

log_f1 = True
checkpoint = True
# Save model even if not best every (epoch % save_interval == 0)
save_interval = 1
model_name = tmp_model
# print train status every (epoch % print_out_every == 0)
print_out_every = 10
# Path to model checkpoint to continue training from there
# If you don't want this option put the value None
continue_from_model_checkpoint = None
delete_model_dir = False

[Device]
enable_gpu = False
